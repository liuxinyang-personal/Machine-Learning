# 1 介绍
吴恩达认为，不断学习与更新基础知识是十分重要的。与其他技术领域一样，机器学习领域也在不断发展，但有些基础算法与核心思想的贡献是经得起时间考验的：
- 算法：线性和逻辑回归、决策树等；
- 概念：正则化、优化损失函数、偏差/方差等；

吴恩达对六种基础算法的来源、用途等进行了详细的讲解，分别是：线性回归、逻辑回归、梯度下降、神经网络、决策树与k均值聚类算法。
# 2 线性回归（Linear Regression）
线性回归是机器学习中的一个关键的统计方法，该算法具有出色的实用性和简便性。

## 2.1 斜率和偏差
当结果与影响它的变量之间的关系遵循直线线性关系时，线性回归很有用。例如，汽车的油耗与其重量成线性关系。

汽车的油耗 $y$ 与其重量 $x$ 之间的关系取决于直线的斜率 $w$（油耗随重量上升的幅度）和偏置项 $b$（零重量时的油耗），以此建立线性数学模型：$y=w\times{x}+b$。

根据上述线性模型，在训练期间给定汽车的重量，算法会预测预期的油耗。它比较了预期和实际的油耗，并将两个油耗的平方差最小化。通常是通过普通最小二乘技术，获得 $w$ 和 $b$ 的最优解。
## 2.2 特征选择与正则化
数据永远不会被完美地衡量，变量之间的重要性并不一样。例如，带有正则化（$L_2$正则化）的线性回归（也称为\[岭回归（ridge regression）\]）鼓励线性回归模型不要过分地考虑所有的变量，应当在模型中选择并考虑更重要的变量。如果为了简单起见，另一种形式的正则化（$L_1$正则化）会产生 lasso 估计（压缩估计），这种算法鼓励尽可能多的特征的系数为零。换句话说，它学会选择具有高预测能力的变量并忽略其余的特征变量，$L_2$只是降低不重要特征的作用。弹性网络结合了这两种类型的正则化，当数据稀疏或特征看起来相关时，它很有用。
## 2.3 线性模型与神经元
传统的神经网络模型中常常会使用线性回归模型作为神经元类型，使得线性回归成为深度学习的基本组成部分。
# 3 逻辑回归（Logistic Regression）
不同于线性回归模型，逻辑回归更多的实现分类任务，但是逻辑回归可以同时用于连续变量和离散型变量。逻辑回归这种利用已知的连续或者离散的样本数据去预测新的样本的概率分布并进行分类的能力，使得逻辑回归成为机器学习以及神经网络中重要的组成部分。
## 3.1 拟合函数
逻辑回归将逻辑函数拟合到数据集，以便预测给定事件（例如，摄入的毒素）发生特定结果（例如，过早死亡）的概率。
## 3.2 最大似然估计
不同于线性回归可以使用
